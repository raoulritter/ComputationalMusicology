---
title: "Portfolio Computational Musicology 2022 Kanye West's Albums and the stages of grief"
date: "Raoul Ritter"
output: 
  flexdashboard::flex_dashboard:
    storyboard: true
    self_contained: false
    theme: 
      version: 4
      bootswatch: minty
---
```{r setup}
library(tidyverse)
library(tidymodels)
library(plotly)
library(protoclust)
library(spotifyr)
library(compmus)
library(flexdashboard)
library(tidyverse)
library(ggdendro)
library(kknn)
library(ggheatmap)
library(protoclust)

TLOP <- get_playlist_audio_features("","6xMmY2bJcoRfTYnbaswB81")
DONDA <- get_playlist_audio_features("","6CBjIOCvkEnNom6zr2CzZt")
YE <- get_playlist_audio_features("","3ttuqnv3RBZ2tT6Fommpyf")
JIK <- get_playlist_audio_features("","3Zea55ELBEcnURbXY1uH3P")
MBDTF <- get_playlist_audio_features("","38LsK6yyXI0apx7n7bbx8i")
YEEZUS <- get_playlist_audio_features("","753uZalNXz3Lv6FreyTWiF")
HEARTBREAKS <- get_playlist_audio_features("","3nH7CBa7ohrAU9k2t7aMgM")
ALL <- rbind(TLOP,DONDA,JIK,MBDTF,YEEZUS,HEARTBREAKS,YE)
```
### Introduction

For my corpus I will be looking at the 7 (solo) studio albums by Kanye West that he created after the death of his mother. Online there is often a theory that these albums correspond with the stages of grief. During this assignment I would like to check if these albums indeed follow this pattern.

1. Shock (808â€™s and Heartbreak)
2. Denial (My Beautiful Dark Twisted Fantasy)
3. Anger (Yeezus)
4. Bargaining (The Life of Pablo or TLOP)
5. Depression (Ye)
6. Testing (JESUS IS KING)
7. Acceptance (DONDA)

In this project I will be analyzing these albums using the information that the spotify API provides such as valence, tempo and speechiness. These variables will be used to analyse if these albums can be categorized as having unique features that correspond to the stages of grief.

The corpus consists of exactly 100 songs. Which knowing Ye is not an accident. The corpus has a large variety of songs and feelings across the different albums. The difficulty in this corpus is if the albums are truly unique or if the diversity of features in songs per album are too large to distinguish.  

Some songs that stand out in this corpus are Power from the album Denial which comes from the album My Beautiful Dark Twisted Fantasy. Which comes off as quite a angry song as well. Issues could also stem from the fact that Ye has consistently made changes to his albums. One example of this is for TLOP in which he changed multiple segments in songs as well as completely adding a new song Saint Pablo to the album. However the issue is that spotify only has the most recent version of his album. Therefore it is difficult to recognize if the changes he has made on a single album also reflect the changes in his process.

### Global Overview of All Albums

```{r fig.height = 10, fig.width = 12}
ALL %>% 
ggplot(aes(x = valence, y = energy, color = playlist_name)) +
  geom_jitter() +
  geom_vline(xintercept = 0.5) +
  geom_hline(yintercept = 0.5) +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 1)) +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 1)) +
  annotate('text', 0.25 / 2, 0.95, label = "Turbulent/Angry", fontface = "bold") +
  annotate('text', 1.75 / 2, 0.95, label = "Happy/Joyful", fontface = "bold") +
  annotate('text', 1.75 / 2, 0.05, label = "Chill/Peaceful", fontface = "bold") +
  annotate('text', 0.25 / 2, 0.05, label = "Sad/Depressing", fontface = "bold")

```

***

This graph gives a global analysis of the overall atmosphere of the music of the 7 Studio Albums that Kanye West released after his mothers death. 

According to the Spotify API, **Energy** is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy.

**Valence** is a measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).

The interesting thing we see at first glance is that there is quite an even distribution between the different albums. They are not specifically as select into their specific segments as we might have thought. However on closer inspection you can see that there are clear differences between certain albums. For example Yeezus which we would associate with anger has most of it's song in the Turbulent/Angry section. Similar case for MBDTF. However Donda has a very diverse spectrum being in three different segments and not having real clustering. However it could also be that due to the style of Kanye West's music largely being Hip Hop these songs are often seen as being in the Angry due to being higher in energy and lower in valance resulting in this clustering. 

<iframe src="https://open.spotify.com/embed/playlist/2benrUkgKv28fkKoBowcFU?utm_source=generator" width="100%" height="380" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>




### The Differences in Valance and Energy over Time


```{r fig.height = 15, fig.width = 15}
ALL <-
  TLOP %>%
  mutate(country = "TLOP") %>%
  bind_rows(DONDA %>% mutate(country = "DONDA")) %>%
  bind_rows(YE %>% mutate(country = "Ye")) %>%
  bind_rows(JIK %>% mutate(country = "Jesus is King")) %>%
  bind_rows(MBDTF %>% mutate(country = "My beautiful dark twisted fantasy")) %>%
  bind_rows(YEEZUS %>% mutate(country = "Yeezus")) %>%
  bind_rows(HEARTBREAKS %>% mutate(country = "808's and Heartbreaks")) %>%
  mutate(
    country = fct_relevel(country, "TLOP", "Donda", "Ye", "Jesus is King", "My beautiful dark twisted fantasy", "Yeezus","808's and Heartbreaks")
  )

difference_albums <-
  ALL %>%
  ggplot(                          # Set up the plot.
    aes(
      x = valence,
      y = energy,
      size = track.popularity,
      colour = danceability,
      label = track.name           # Labels will be interactively visible.
    )
  ) +
  geom_point() +                   # Scatter plot.
  geom_rug(size = 0.1) +           # Add 'fringes' to show data distribution.
  facet_wrap(~country) +           # Separate charts per country.
  scale_x_continuous(              # Fine-tune the x axis.
    limits = c(0, 1),
    breaks = c(0, 0.50, 1),        # Use grid-lines for quadrants only.
    minor_breaks = NULL            # Remove 'minor' grid-lines.
  ) +
  scale_y_continuous(              # Fine-tune the y axis in the same way.
    limits = c(0, 1),
    breaks = c(0, 0.50, 1),
    minor_breaks = NULL
  ) +
  scale_colour_viridis_c(          # Use the cividis palette
    option = "E",                  # Qualitative set.
    alpha = 0.8,                   # Include some transparency
    guide = "none"
  ) +
  scale_size_continuous(           # Fine-tune the sizes of each point.
    guide = "none"                 # Remove the legend for size.
  ) +
  theme_dark() +                  # Use a simpler theme.
  labs(                            # Make the titles nice.
    x = "Valence",
    y = "Energy"
  )

ggplotly(difference_albums)

```
***
When we take a look at the individual albums more clearly it becomes clear that there are distinct differences but it is quite difficult to digest from this information if this is indeed caused by the stages of grief or if these are more based on the individual tracks and semantics in each. The Albums all cover a large variety of the spectrum. However we do see clear examples of the Album Ye where the Valance of most songs is much lower then an album such as TLOP which we would associate with bargaining and having a large spectrum of different energies and valances. Therefore the hypothesis about the different stages of grief appear to have some truth in the fact that we see differences between different albums which correspond to differences in energy and valance. 



### Self-Similarity


```{r Pitches}
FG <-
  get_tidy_audio_analysis("2QpGZOhTCHHiKmpSO9FW4h") %>% # Change URI.
  compmus_align(bars, segments) %>%                     # Change `bars`
  select(bars) %>%                                      #   in all three
  unnest(bars) %>%                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) %>%
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )
FG %>%
  compmus_self_similarity(timbre, "cosine") %>% 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "") + ggtitle("Follow God from JIK by Kanye West")

hurricane <-
  get_tidy_audio_analysis("6Hfu9sc7jvv6coyy2LlzBF") %>% # Change URI.
  compmus_align(bars, segments) %>%                     # Change `bars`
  select(bars) %>%                                      #   in all three
  unnest(bars) %>%                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) %>%
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )
hurricane %>%
  compmus_self_similarity(timbre, "cosine") %>% 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "") + ggtitle("Hurricane from the album Donda by Kanye West")

heartless <-
  get_tidy_audio_analysis("4EWCNWgDS8707fNSZ1oaA5") %>% # Change URI.
  compmus_align(bars, segments) %>%                     # Change `bars`
  select(bars) %>%                                      #   in all three
  unnest(bars) %>%                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) %>%
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )
heartless %>%
  compmus_self_similarity(timbre, "cosine") %>% 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "") + ggtitle("Heartless from the album 808's and Heartbreaks by Kanye West")

bs <-
  get_tidy_audio_analysis("722tgOgdIbNe3BEyLnejw4") %>% # Change URI.
  compmus_align(bars, segments) %>%                     # Change `bars`
  select(bars) %>%                                      #   in all three
  unnest(bars) %>%                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) %>%
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )
bs %>%
  compmus_self_similarity(timbre, "cosine") %>% 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "") + ggtitle("Black Skinhead from the album Yeezus by Kanye West")

power <-
  get_tidy_audio_analysis("2gZUPNdnz5Y45eiGxpHGSc") %>% # Change URI.
  compmus_align(bars, segments) %>%                     # Change `bars`
  select(bars) %>%                                      #   in all three
  unnest(bars) %>%                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) %>%
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )
power %>%
  compmus_self_similarity(timbre, "cosine") %>% 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "") + ggtitle("Power from the album MBDTF by Kanye West")

allmine <-
  get_tidy_audio_analysis("4KW1lqgSr8TKrvBII0Brf8") %>% # Change URI.
  compmus_align(bars, segments) %>%                     # Change `bars`
  select(bars) %>%                                      #   in all three
  unnest(bars) %>%                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) %>%
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )
allmine %>%
  compmus_self_similarity(timbre, "cosine") %>% 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "") + ggtitle("All mine from the album Ye  by Kanye West")

FSMH1 <-
  get_tidy_audio_analysis("3U21A07gAloCc4P7J8rxcn") %>% # Change URI.
  compmus_align(bars, segments) %>%                     # Change `bars`
  select(bars) %>%                                      #   in all three
  unnest(bars) %>%                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) %>%
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )
FSMH1 %>%
  compmus_self_similarity(timbre, "cosine") %>% 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "") + ggtitle("Father Stretch My Hands Pt.1 from TLOP by Kanye West")


```


***
For this visualization I wanted to see if there was a formula for what creates a hit Kanye west song. Here the Self Similarity matrix are plotted for each of the most popular song per album. We see that there are quite some differences in these visualisations. 


In a self-similarity matrix, yellow shows variance and more blue shows similarity. These self similarity matrices focus on 1) timbre and on 2) pitches. According to the [Spotify API](https://developer.spotify.com/documentation/web-api/reference/#/operations/get-audio-analysis). The first is **timbre** . The second self similarity matrix is  **pitch**. What we see is that for specific songs there are very high numbers of self similarity due to all songs having large blocks with one exception. This song being All mine. This is also something that corresponds with the song. There are many unexpected parts and it is not a song that has the same sentiment throughout it. This also fits with our hypothesis of this album being associated with a more depressed segment of his grieving process. 


### What makes a hit Kanye west song


```{r hits}
hits <- 
    get_playlist_audio_features('Kanye Hits', '1y8HgLaiz4E3Vh52WyErei') %>% 
    add_audio_analysis %>% 
    mutate(
        segments = 
            map2(segments, key, compmus_c_transpose)) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'mean', norm = 'manhattan'),
        timbre =
            map(
                segments,
                compmus_summarise, timbre,
                method = 'mean')) %>% 
    mutate(pitches = map(pitches, compmus_normalise, 'clr')) %>% 
    mutate_at(vars(pitches, timbre), map, bind_rows) %>% 
    unnest(cols = c(pitches, timbre))
hit_juice <- 
    recipe(track.name ~
               danceability +
               energy +
               loudness +
               speechiness +
               acousticness +
               instrumentalness +
               liveness +
               valence +
               tempo +
               duration +
               C + `C#|Db` + D + `D#|Eb` +
               E + `F` + `F#|Gb` + G +
               `G#|Ab` + A + `A#|Bb` + B +
               c01 + c02 + c03 + c04 + c05 + c06 +
               c07 + c08 + c09 + c10 + c11 + c12,
           data = hits) %>% 
    step_range(all_predictors()) %>% 
    prep(hits %>% mutate(track.name = str_trunc(track.name, 20))) %>% 
    juice %>% 
    column_to_rownames('track.name')
ggheatmap(
    hit_juice,
    dist_method = 'manhattan'
)
```

***
The deprogram and heat map in this segment breaks down the different hits per Album. However we once again see that the vital classifier lyrics is missing. 


### Classifying the albums 

```{r classification}

TLOP <- get_playlist_audio_features("","6xMmY2bJcoRfTYnbaswB81")
DONDA <- get_playlist_audio_features("","6CBjIOCvkEnNom6zr2CzZt")
YE <- get_playlist_audio_features("","3ttuqnv3RBZ2tT6Fommpyf")

pop <- 
    get_playlist_audio_features('spotify', '6xMmY2bJcoRfTYnbaswB81') %>% 
    add_audio_analysis
party <- 
    get_playlist_audio_features('spotify', '6CBjIOCvkEnNom6zr2CzZt') %>% 
    add_audio_analysis
workout <- 
    get_playlist_audio_features('spotify', '3ttuqnv3RBZ2tT6Fommpyf') %>% 
    add_audio_analysis
indie <- 
    pop %>% mutate(playlist = "TLOP") %>% 
    bind_rows(
        party %>% mutate(playlist = "Donda"),
        workout %>% mutate(playlist = "Ye")) %>% 
    mutate(playlist = factor(playlist)) %>% 
    mutate(
        segments = 
            map2(segments, key, compmus_c_transpose)) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'mean', norm = 'manhattan'),
        timbre =
            map(
                segments,
                compmus_summarise, timbre,
                method = 'mean')) %>% 
    mutate(pitches = map(pitches, compmus_normalise, 'clr')) %>% 
    mutate_at(vars(pitches, timbre), map, bind_rows) %>% 
    unnest(cols = c(pitches, timbre))
indie_juice <- 
    recipe(playlist ~
               danceability +
               energy +
               loudness +
               speechiness +
               acousticness +
               instrumentalness +
               liveness +
               valence +
               tempo +
               duration +
               C + `C#|Db` + D + `D#|Eb` +
               E + `F` + `F#|Gb` + G +
               `G#|Ab` + A + `A#|Bb` + B +
               c01 + c02 + c03 + c04 + c05 + c06 +
               c07 + c08 + c09 + c10 + c11 + c12,
           data = indie) %>% 
    step_center(all_predictors()) %>%
    step_scale(all_predictors()) %>%
    # step_range(all_predictors()) %>% 
    prep(indie) %>% 
    juice
indie_cv <- indie_juice %>% vfold_cv(10)
indie_knn <- 
  nearest_neighbor(mode = 'classification', neighbors = 1) %>% 
  set_engine('kknn')
predict_knn_reduced <- function(split) {
    fit(
        indie_knn, 
        playlist ~ c01 + liveness + acousticness + c02 + energy, 
        data = analysis(split)) %>% 
    predict(assessment(split), type = 'class') %>%
    bind_cols(assessment(split))
}
indie_cv %>% 
    mutate(pred = map(splits, predict_knn_reduced)) %>% unnest(pred) %>% 
    conf_mat(truth = playlist, estimate = .pred_class) %>% 
    autoplot(type = 'mosaic')
```

***
The mosaic at the left shows the performance of a classifier trying to distinguish the three most recent studio albums: TLOP, Ye, and Donda. Using a random-forest classifier, the most important features for classifying tracks among these playlists are:

Timbre Component 1 (loudness),
Timbre Component 2 (brightness),
liveness,
acousticness, and
energy.

###Conclusion

***
During this assignment I found that I had a very interesting corpus. However the larger struggle I had aswell as many of my students is simply that the Spotify API is not powerful enough to truly capture the sentiment that specific songs have. This is something that the human ear is very good at as a form of musicial information retrieval. As we as humans are able to digest lyrical meaning and acoustics at the same time. This is a feature that the Spotify API lacks. Therefore it would be very interesting to see if different techniques would be better suited to grasp the sentiment of Kanye West's songs. This could be done by looking at the lyrics from all the songs he has produced since his mothers death and looking at their sentiment and what mood comes from this sentiment analysis. That way we would be better be able to tell if the popular fan theory of the albums corresponding with the stages of grief can truly be measured.  


<iframe src="https://open.spotify.com/embed/playlist/2benrUkgKv28fkKoBowcFU?utm_source=generator" width="100%" height="380" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>
